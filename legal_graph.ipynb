{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "neo4j-graphrag 1.6.1 requires json-repair<0.40.0,>=0.39.1, but you have json-repair 0.42.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-community openai langchain-experimental neo4j tiktoken yfiles_jupyter_graphs python-dotenv json-repair langchain-openai langchain_core PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import  RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from neo4j import GraphDatabase\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "import os\n",
    "from neo4j import  Driver\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener variables de entorno\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "azure_deployment = os.getenv(\"GPT_ENGINE\")\n",
    "api_version = os.getenv(\"API_VERSION\")\n",
    "neo4j_url = os.getenv(\"NEO4J_URL\")\n",
    "neo4j_username = os.getenv(\"NEO4J_USERNAME\")\n",
    "neo4j_password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "azure_embedding_model=os.getenv(\"AZURE_EMBEDDING_MODEL\")\n",
    "embedding_deployment=os.getenv(\"EMBEDDING_DEPLOYMENT\")\n",
    "openai_api_type=os.getenv(\"OPEN_API_TYPE\")\n",
    "embedding_openai_api_version=os.getenv(\"EMBEDDING_OPEN_API_VERSION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(url=neo4j_url, username=neo4j_username, password=neo4j_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES_FOLDER_PATH=\"./material\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "documents = []\n",
    "\n",
    "# Verifica si la carpeta existe\n",
    "if os.path.isdir(FILES_FOLDER_PATH):\n",
    "    for filename in os.listdir(FILES_FOLDER_PATH):\n",
    "        file_path = os.path.join(FILES_FOLDER_PATH, filename)\n",
    "\n",
    "        # Verifica si el archivo es un PDF\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            try:\n",
    "                with open(file_path, \"rb\") as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    text = \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "                    documents.append({\"filename\": filename, \"content\": text})\n",
    "                    print(f\"Text extracted from: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"The folder '{FILES_FOLDER_PATH}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    print(f\"{doc['filename']} - Length of content: {len(doc['content'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Configurar el splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=24)\n",
    "\n",
    "# Convertir los documentos en chunks\n",
    "chunked_documents = []\n",
    "for doc in documents:\n",
    "    chunks = text_splitter.split_text(doc[\"content\"])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunked_documents.append(Document(page_content=chunk, metadata={\"filename\": doc[\"filename\"], \"chunk_id\": i + 1}))\n",
    "\n",
    "print(len(chunked_documents))\n",
    "print(chunked_documents[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "# # Definir los nodos y relaciones permitidos para la extracción de información\n",
    "allowed_nodes = []\n",
    "allowed_relationships = []\n",
    "\n",
    "\n",
    "# Configurar Azure OpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    openai_api_key=azure_api_key,\n",
    "    azure_deployment=azure_deployment,\n",
    "    model=azure_deployment,  \n",
    "    api_version=api_version,  \n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Configurar el transformador de gráficos con el prompt\n",
    "llm_transformer = LLMGraphTransformer(\n",
    "    llm=llm,\n",
    "#     allowed_nodes=allowed_nodes,\n",
    "#     allowed_relationships=allowed_relationships,\n",
    " )\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(chunked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUIDADO! Para resetear el grafo en caso de ser necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Conectar a la base de datos Neo4j\n",
    "driver = GraphDatabase.driver(uri=neo4j_url, auth=(neo4j_username, neo4j_password))\n",
    "\n",
    "# Función para limpiar el grafo\n",
    "def clear_graph(tx):\n",
    "    tx.run(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "# Función para comprobar si el grafo está vacío (contar nodos y relaciones)\n",
    "def check_empty_graph(tx):\n",
    "    # Contar tanto los nodos como las relaciones\n",
    "    result = tx.run(\"MATCH (n) OPTIONAL MATCH (n)-[r]->() RETURN count(n) AS node_count, count(r) AS relationship_count\")\n",
    "    for record in result:\n",
    "        return record[\"node_count\"], record[\"relationship_count\"]\n",
    "\n",
    "# Ejecutar la limpieza del grafo\n",
    "def execute_clear_graph():\n",
    "    with driver.session() as session:\n",
    "        session.execute_write(clear_graph)\n",
    "        print(\"Graph database cleared successfully.\")\n",
    "\n",
    "# Ejecutar la comprobación del estado del grafo\n",
    "def execute_check_empty_graph():\n",
    "    with driver.session() as session:\n",
    "        node_count, relationship_count = session.execute_read(check_empty_graph)\n",
    "        print(f\"Node count: {node_count}, Relationship count: {relationship_count}\")\n",
    "\n",
    "# Limpiar el grafo y luego verificar\n",
    "execute_clear_graph()          # Limpiar el grafo\n",
    "execute_check_empty_graph()    # Verificar que está vacío\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_graph_documents(\n",
    "    graph_documents,\n",
    "    baseEntityLabel=True,\n",
    "    include_source=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showGraph():\n",
    "    driver = GraphDatabase.driver(\n",
    "        uri = neo4j_url,\n",
    "        auth = (neo4j_username,\n",
    "                neo4j_password)\n",
    "    )\n",
    "    session = driver.session()\n",
    "    widget = GraphWidget(graph=session.run(\"MATCH (s)-[r:!MENTIONS]->(t) RETURN s,r,t\").graph())\n",
    "    widget.node_label_mapping = 'id'\n",
    "    return widget\n",
    "\n",
    "showGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "# Configurar Embeddings de Azure OpenAI\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=azure_embedding_model,\n",
    "    openai_api_key=azure_api_key,\n",
    "    azure_endpoint=azure_endpoint, \n",
    "    deployment=embedding_deployment, \n",
    "    openai_api_type=openai_api_type,\n",
    "    openai_api_version=embedding_openai_api_version,\n",
    ")\n",
    "\n",
    "# Crear índice vectorial en Neo4j\n",
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    embeddings,\n",
    "    search_type=\"hybrid\",\n",
    "    node_label=\"Document\",\n",
    "    text_node_properties=[\"text\"],\n",
    "    embedding_node_property=\"embedding\",\n",
    "    url=neo4j_url,\n",
    "    username=neo4j_username,\n",
    "    password=neo4j_password,\n",
    ")\n",
    "\n",
    "# Convertir el índice en un Retriever\n",
    "vector_retriever = vector_index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(\n",
    "        uri = neo4j_url, \n",
    "        auth = (neo4j_username,\n",
    "                neo4j_password)\n",
    "    )\n",
    "\n",
    "def create_fulltext_index(tx):\n",
    "    query = '''\n",
    "    CREATE FULLTEXT INDEX `fulltext_entity_id` \n",
    "    FOR (n:__Entity__) \n",
    "    ON EACH [n.id];\n",
    "    '''\n",
    "    tx.run(query)\n",
    "\n",
    "# Function to execute the query\n",
    "def create_index():\n",
    "    with driver.session() as session:\n",
    "        session.execute_write(create_fulltext_index)\n",
    "        print(\"Fulltext index created successfully.\")\n",
    "\n",
    "# Call the function to create the index\n",
    "try:\n",
    "    create_index()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Close the driver connection\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "# Configurar Azure OpenAI\n",
    "llmGraph= AzureChatOpenAI(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    openai_api_key=azure_api_key,\n",
    "    azure_deployment=azure_deployment,\n",
    "    model=azure_deployment,\n",
    "    api_version=api_version,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "\n",
    "# Usar GraphCypherQAChain para integrar las consultas al gráfico y la LLM\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    llmGraph,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True,\n",
    "    validate_cypher=True\n",
    ")\n",
    "\n",
    "# Función para hacer las preguntas all llm contra el graph\n",
    "def ask_question(query):\n",
    "    response = chain.invoke({\"query\": query})\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que ocurre por detras:\n",
    "1. Conuslta el schema del grafo, de esta forma puede encontrar una query adecuada\n",
    "2. Genera la consulta cypher.\n",
    "3. Realiza una valdiaciòn de la conuslta cypher_query_corrector.\n",
    "4. Ejecuta la consulta contra el grafo.\n",
    "5. Una ultima consulta a un llm para obtener el reusltado final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_retriever(query: str):\n",
    "    graph_data = ask_question(query)\n",
    "    vector_data = [el.page_content for el in vector_retriever.invoke(query)]\n",
    "    final_data = f\"\"\"Graph data:\n",
    "{graph_data}\n",
    "vector data:\n",
    "{\"#Document \".join(vector_data)}\n",
    "    \"\"\"\n",
    "    print(final_data)\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_final_answer = AzureChatOpenAI(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    openai_api_key=azure_api_key,\n",
    "    azure_deployment=azure_deployment,\n",
    "    model=azure_deployment,\n",
    "    api_version=api_version,\n",
    "    temperature= 0\n",
    ")\n",
    "\n",
    "final_prompt_structure = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are AI assistant and you will answers questions using graph data and the vector data.\"),\n",
    "        (\"human\", \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Use natural language and be concise.\n",
    "Answer:\"\"\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3. Crea una función simple para procesar la entrada\n",
    "def process_input(query):\n",
    "    # Obtener el contexto usando el retriever\n",
    "    context = full_retriever(query)\n",
    "    \n",
    "    # Preparar los datos para el prompt\n",
    "    data = {\n",
    "        \"context\": context,\n",
    "        \"query\": query\n",
    "    }\n",
    "\n",
    "    final_prompt = final_prompt_structure.format(**data)\n",
    "    response = llm_final_answer.invoke(final_prompt)\n",
    "    return response.text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\n",
    "response = process_input(query)\n",
    "print(\"--------FINAL ANSWER--------\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
